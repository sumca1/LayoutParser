"""
× ×™×ª×•×— ××•×“×œ×™ Dicta - ××” ×›×œ ××—×“ ×¢×•×©×”?
"""

from huggingface_hub import list_models, model_info

print("ğŸ” × ×™×ª×•×— ××¤×•×¨×˜ ×©×œ ××•×“×œ×™ Dicta\n")
print("=" * 80)

models = list(list_models(author="dicta-il"))

# ×§×˜×’×•×¨×™×–×¦×™×” ××¤×•×¨×˜×ª
analysis = {
    "××•×“×œ×™ ×‘×¡×™×¡ (Base Models)": {
        "models": [],
        "description": "××•×“×œ×™ BERT ×‘×¡×™×¡×™×™× - ×”×‘×¡×™×¡ ×œ×›×œ ×”×©××¨",
        "use": "×‘×“×¨×š ×›×œ×œ ×œ× ××©×ª××©×™× ×‘×”× ×™×©×™×¨×•×ª"
    },
    "× ×™×§×•×“ (Diacritization)": {
        "models": [],
        "description": "×”×•×¡×¤×ª × ×™×§×•×“ ×œ×¢×‘×¨×™×ª",
        "use": "×©×™××•×© ×™×©×™×¨ - ××•×¡×™×£ × ×™×§×•×“ ×œ×˜×§×¡×˜"
    },
    "× ×™×ª×•×— ××•×¨×¤×•×œ×•×’×™ (Morphology)": {
        "models": [],
        "description": "×©×•×¨×©, ×‘× ×™×™×Ÿ, ×–××Ÿ, ×’×•×£",
        "use": "××©×œ×™× ×œ× ×™×§×•×“ - ××–×”×” ×¦×•×¨×ª ×”××™×œ×”"
    },
    "× ×™×ª×•×— ×œ×§×¡×™×§×œ×™ (Lexical)": {
        "models": [],
        "description": "××©××¢×•×ª ××™×œ×™×, × ×™×ª×•×— ×¡×× ×˜×™",
        "use": "××©×œ×™× ×œ× ×™×§×•×“ - ×”×§×©×¨ ××©××¢×•×ª×™"
    },
    "×–×™×”×•×™ ×™×©×•×™×•×ª (NER)": {
        "models": [],
        "description": "×–×™×”×•×™ ×©××•×ª, ××§×•××•×ª, ××¨×’×•× ×™×",
        "use": "×¢×™×‘×•×“ ×˜×§×¡×˜ - ×œ× ×¨×œ×•×•× ×˜×™ ×œ× ×™×§×•×“"
    },
    "×¤×¨×¡×•×¨ ×ª×—×‘×™×¨×™ (Parsing)": {
        "models": [],
        "description": "××‘× ×” ××©×¤×˜, ×™×—×¡×™ ×ª×œ×•×ª",
        "use": "××©×œ×™× ×œ× ×™×§×•×“ - ×”×‘× ×ª ×ª×¤×§×™×“ ×‘×¤×•×¢×œ"
    },
    "×¡× ×˜×™×× ×˜ (Sentiment)": {
        "models": [],
        "description": "×–×™×”×•×™ ×¨×’×©×•×ª ×‘×˜×§×¡×˜",
        "use": "×œ× ×¨×œ×•×•× ×˜×™ ×œ× ×™×§×•×“"
    },
    "××•×“×œ×™ ×©×¤×” (LLM)": {
        "models": [],
        "description": "××•×“×œ×™× ×’×“×•×œ×™× - ×”×‘× ×” ××œ××”",
        "use": "×™×›×•×œ×™× ×œ×¢×©×•×ª ×”×›×œ ××‘×œ ×›×‘×“×™× ×××•×“"
    },
    "××•×“×œ×™× ××™×•×—×“×™×": {
        "models": [],
        "description": "BEREL (×¨×‘× ×™), segmentation, ×•×›×•'",
        "use": "×©×™××•×© ×¡×¤×¦×™×¤×™"
    }
}

for model in models:
    name = model.modelId.split('/')[-1]
    
    if 'menaked' in name.lower() or 'nakdan' in name.lower():
        analysis["× ×™×§×•×“ (Diacritization)"]["models"].append(name)
    elif 'morph' in name.lower():
        analysis["× ×™×ª×•×— ××•×¨×¤×•×œ×•×’×™ (Morphology)"]["models"].append(name)
    elif 'lex' in name.lower():
        analysis["× ×™×ª×•×— ×œ×§×¡×™×§×œ×™ (Lexical)"]["models"].append(name)
    elif 'ner' in name.lower():
        analysis["×–×™×”×•×™ ×™×©×•×™×•×ª (NER)"]["models"].append(name)
    elif 'parse' in name.lower() or 'syntax' in name.lower():
        analysis["×¤×¨×¡×•×¨ ×ª×—×‘×™×¨×™ (Parsing)"]["models"].append(name)
    elif 'sentiment' in name.lower():
        analysis["×¡× ×˜×™×× ×˜ (Sentiment)"]["models"].append(name)
    elif 'dictalm' in name.lower():
        analysis["××•×“×œ×™ ×©×¤×” (LLM)"]["models"].append(name)
    elif any(x in name.lower() for x in ['berel', 'seg', 'heq', 'splinter', 'joint']):
        analysis["××•×“×œ×™× ××™×•×—×“×™×"]["models"].append(name)
    else:
        analysis["××•×“×œ×™ ×‘×¡×™×¡ (Base Models)"]["models"].append(name)

# ×”×¦×’ × ×™×ª×•×—
for category, data in analysis.items():
    if data["models"]:
        print(f"\nğŸ“¦ {category}")
        print(f"   ğŸ“ {data['description']}")
        print(f"   ğŸ’¡ {data['use']}")
        print(f"   ğŸ“Š ××•×“×œ×™×: {len(data['models'])}")
        print()
        for m in sorted(data["models"])[:5]:  # ×”×¦×’ ×¨×§ 5 ×¨××©×•× ×™×
            print(f"      â€¢ {m}")
        if len(data["models"]) > 5:
            remaining = len(data["models"]) - 5
            print(f"      ... ×•×¢×•×“ {remaining}")

print("\n" + "=" * 80)
print("ğŸ¯ ××•×“×œ×™× ××©×•×œ×‘×™× (×¢×•×©×™× ×›××” ×“×‘×¨×™× ×‘×™×—×“)")
print("=" * 80)

combined = {
    "dictabert-joint": "× ×™×ª×•×— ××•×¨×¤×•×œ×•×’×™ + ×ª×—×‘×™×¨×™ ×‘×™×—×“",
    "dictabert-tiny-joint": "×’×¨×¡×” ×§×˜× ×” ×©×œ joint",
    "DictaLM-3.0-*": "×¢×•×©×” ×”×›×œ - × ×™×§×•×“, ××•×¨×¤×•×œ×•×’×™×”, ×”×‘× ×” (××‘×œ ×›×‘×“!)",
    "BEREL": "×›××• BERT ××‘×œ ××™×•×—×“ ×œ×¢×‘×¨×™×ª ×¨×‘× ×™×ª"
}

for model, capability in combined.items():
    print(f"\nâœ… {model}")
    print(f"   â†’ {capability}")

print("\n" + "=" * 80)
print("ğŸ’¡ ××” ×›×“××™ ×œ×”×•×¨×™×“ ×œ×©×™×¤×•×¨ × ×™×§×•×“?")
print("=" * 80)

recommendations = [
    {
        "model": "dictabert-large-char-menaked",
        "status": "âœ… ×›×‘×¨ ×™×© ×œ×š!",
        "priority": "1 (×—×•×‘×”)",
        "size": "~1.2GB"
    },
    {
        "model": "dictabert-morph",
        "status": "ğŸ”„ × ×•×¨×™×“ ×¢×›×©×™×•",
        "priority": "2 (××•××œ×¥ ×××•×“)",
        "size": "~400MB"
    },
    {
        "model": "dictabert-lex",
        "status": "â­ ××•×¤×¦×™×•× ×œ×™",
        "priority": "3 (×©×™×¤×•×¨ × ×•×¡×£)",
        "size": "~400MB"
    },
    {
        "model": "dictabert-joint",
        "status": "â­ ×—×œ×•×¤×” ×œ-morph",
        "priority": "2-3 (×‘××§×•× morph+parse)",
        "size": "~400MB"
    },
    {
        "model": "DictaLM-3.0-1.7B",
        "status": "ğŸ’ª ×× ×™×© GPU",
        "priority": "4 (×¨×§ ×× ×™×© ××©××‘×™×)",
        "size": "~3.5GB"
    }
]

print("\n×¡×“×¨ ×¢×“×™×¤×•×™×•×ª:")
for i, rec in enumerate(recommendations, 1):
    print(f"\n{i}. {rec['model']}")
    print(f"   ×¡×˜×˜×•×¡: {rec['status']}")
    print(f"   ×¢×“×™×¤×•×ª: {rec['priority']}")
    print(f"   ×’×•×“×œ: {rec['size']}")

print("\n" + "=" * 80)
print("ğŸ¯ ×¡×™×›×•×:")
print("=" * 80)
print("""
ğŸ”¢ 60 ××•×“×œ×™× ××ª×—×œ×§×™× ×œ:
   â€¢ ~15 ××•×“×œ×™ ×‘×¡×™×¡ (BERT ×’× ×¨×™)
   â€¢ ~30 ×’×¨×¡××•×ª ×©×œ DictaLM (1.7B, 12B, 24B + quantized)
   â€¢ ~15 ××•×“×œ×™× ×¡×¤×¦×™×¤×™×™× (× ×™×§×•×“, NER, parse, ×•×›×•')

ğŸ­ ×›×œ ××•×“×œ ×¢×•×©×” ××©×”×• ×¡×¤×¦×™×¤×™, ××‘×œ:
   â€¢ dictabert-joint - ×¢×•×©×” ××•×¨×¤×•×œ×•×’×™×” + ×ª×—×‘×™×¨ ×‘×™×—×“
   â€¢ DictaLM - ×¢×•×©×” ×”×›×œ (××‘×œ ×›×‘×“ ×××•×“)
   
ğŸ’¡ ×œ×©×™×¤×•×¨ × ×™×§×•×“:
   1. dictabert-large-char-menaked (×™×© ×œ×š âœ“)
   2. dictabert-morph (××•×¨×™×“ ×¢×›×©×™×• ğŸ”„)
   3. dictabert-lex (××•×¤×¦×™×•× ×œ×™)
   
ğŸ“Š ×¦×¤×™ ×©×™×¤×•×¨:
   â€¢ ×¨×§ × ×™×§×•×“: 90% ×“×™×•×§
   â€¢ × ×™×§×•×“ + morph: 95%+ ×“×™×•×§
   â€¢ × ×™×§×•×“ + morph + lex: 97%+ ×“×™×•×§
""")
