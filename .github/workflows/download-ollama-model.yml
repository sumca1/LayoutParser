name: Download Ollama Model

on:
  workflow_dispatch:
    inputs:
      model_name:
        description: '×©× ××•×“×œ Ollama (×œ××©×œ: llama3.1:8b)'
        required: true
        default: 'llama3.1:8b'
      chunk_size_mb:
        description: '×’×•×“×œ chunk ×‘××’×”-×‘×ª×™×'
        required: false
        default: '1900'

jobs:
  download-and-upload:
    runs-on: ubuntu-latest
    timeout-minutes: 360  # 6 ×©×¢×•×ª ××§×¡×™××•×
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          
      - name: Install dependencies
        run: |
          pip install requests tqdm
          
      - name: Download from Ollama Registry
        run: |
          MODEL_NAME="${{ github.event.inputs.model_name }}"
          MODEL_FOLDER="${MODEL_NAME//:/_}"
          
          echo "ğŸ“¥ ××•×¨×™×“ $MODEL_NAME ×-Ollama Registry..."
          
          python3 << 'PYTHON_SCRIPT'
          import requests
          import json
          import os
          from pathlib import Path
          from tqdm import tqdm
          
          model_name = "${{ github.event.inputs.model_name }}"
          model_folder = model_name.replace(':', '_')
          
          # ×™×¦×™×¨×ª ×ª×™×§×™×™×”
          Path(model_folder).mkdir(exist_ok=True)
          
          # ×”×•×¨×“×” ×-Ollama Registry API
          print(f"ğŸ” ××§×‘×œ ××™×“×¢ ×¢×œ {model_name}...")
          
          # Ollama Registry API - manifest
          registry_url = f"https://registry.ollama.ai/v2/library/{model_name.split(':')[0]}/manifests/{model_name.split(':')[1] if ':' in model_name else 'latest'}"
          
          print(f"ğŸ“¡ URL: {registry_url}")
          
          headers = {
              'Accept': 'application/vnd.docker.distribution.manifest.v2+json'
          }
          
          response = requests.get(registry_url, headers=headers)
          
          if response.status_code == 200:
              manifest = response.json()
              print(f"âœ… ×§×™×‘×œ× ×• manifest")
              
              # ×©××™×¨×ª manifest
              with open(f"{model_folder}/manifest.json", 'w') as f:
                  json.dump(manifest, f, indent=2)
              
              # ×”×•×¨×“×ª layers
              total_size = sum(layer['size'] for layer in manifest.get('layers', []))
              print(f"ğŸ“¦ ×¡×”\"×› {len(manifest.get('layers', []))} layers, ×’×•×“×œ: {total_size / (1024**3):.2f} GB")
              
              for i, layer in enumerate(manifest.get('layers', [])):
                  digest = layer['digest']
                  size = layer['size']
                  
                  print(f"\nğŸ“¥ Layer {i+1}/{len(manifest['layers'])}: {digest[:12]}... ({size / (1024**2):.1f} MB)")
                  
                  # ×”×•×¨×“×ª layer
                  blob_url = f"https://registry.ollama.ai/v2/library/{model_name.split(':')[0]}/blobs/{digest}"
                  
                  response = requests.get(blob_url, stream=True)
                  
                  if response.status_code == 200:
                      output_file = f"{model_folder}/layer_{i:03d}_{digest.replace(':', '_')}.blob"
                      
                      with open(output_file, 'wb') as f:
                          for chunk in tqdm(response.iter_content(chunk_size=8192), 
                                          total=size//8192, 
                                          unit='KB',
                                          desc=f"Layer {i+1}"):
                              f.write(chunk)
                      
                      print(f"âœ… ×©××•×¨: {output_file}")
                  else:
                      print(f"âŒ ×©×’×™××” ×‘×”×•×¨×“×ª layer: {response.status_code}")
                      exit(1)
              
              # ×©××™×¨×ª config
              if 'config' in manifest:
                  config_digest = manifest['config']['digest']
                  config_url = f"https://registry.ollama.ai/v2/library/{model_name.split(':')[0]}/blobs/{config_digest}"
                  
                  print(f"\nâš™ï¸ ××•×¨×™×“ config...")
                  response = requests.get(config_url)
                  
                  if response.status_code == 200:
                      with open(f"{model_folder}/config.json", 'wb') as f:
                          f.write(response.content)
                      print(f"âœ… Config × ×©××¨")
              
              print(f"\nğŸ‰ ×”×•×¨×“×” ×”×•×©×œ××”!")
              
          else:
              print(f"âŒ ×©×’×™××”: {response.status_code}")
              print(response.text)
              exit(1)
          
          PYTHON_SCRIPT
          
          echo "âœ… ×”××•×“×œ ×”×•×¨×“ ×‘×”×¦×œ×—×”!"
          
      - name: Create archive
        run: |
          MODEL_NAME="${{ github.event.inputs.model_name }}"
          MODEL_FOLDER="${MODEL_NAME//:/_}"
          
          cd "$MODEL_FOLDER"
          echo "ğŸ“¦ ×™×•×¦×¨ ××¨×›×™×•×Ÿ..."
          tar -czf models.tar.gz *.blob *.json
          rm *.blob *.json
          
          SIZE=$(du -h models.tar.gz | cut -f1)
          echo "âœ… × ×•×¦×¨ ××¨×›×™×•×Ÿ: models.tar.gz ($SIZE)"
          
      - name: Split if needed
        run: |
          MODEL_NAME="${{ github.event.inputs.model_name }}"
          MODEL_FOLDER="${MODEL_NAME//:/_}"
          CHUNK_SIZE="${{ github.event.inputs.chunk_size_mb }}"
          
          cd "$MODEL_FOLDER"
          
          # ×’×•×“×œ ×‘bytes
          SIZE_BYTES=$(stat -f%z models.tar.gz 2>/dev/null || stat -c%s models.tar.gz)
          SIZE_MB=$((SIZE_BYTES / 1048576))
          
          echo "ğŸ“Š ×’×•×“×œ: ${SIZE_MB}MB"
          
          if [ $SIZE_MB -gt $CHUNK_SIZE ]; then
            echo "âœ‚ï¸  ××¤×¦×œ ×œchunks ×©×œ ${CHUNK_SIZE}MB..."
            split -b ${CHUNK_SIZE}m models.tar.gz "part_"
            rm models.tar.gz
            
            echo "âœ… × ×•×¦×¨×• ××§×˜×¢×™×:"
            ls -lh part_*
            
            # ×™×¦×™×¨×ª ×¡×§×¨×™×¤×˜ reassemble bash
            echo '#!/bin/bash' > reassemble.sh
            echo 'cat part_* > models.tar.gz' >> reassemble.sh
            echo 'echo "âœ… ××•×—×“ ×œ-models.tar.gz"' >> reassemble.sh
            echo 'rm part_*' >> reassemble.sh
            chmod +x reassemble.sh
            
            # ×™×¦×™×¨×ª ×¡×§×¨×™×¤×˜ reassemble PowerShell
            echo '# PowerShell reassemble script' > reassemble.ps1
            echo '$parts = Get-ChildItem -Filter "part_*" | Sort-Object Name' >> reassemble.ps1
            echo '$output = "models.tar.gz"' >> reassemble.ps1
            echo '$outStream = [System.IO.File]::OpenWrite($output)' >> reassemble.ps1
            echo 'foreach ($part in $parts) {' >> reassemble.ps1
            echo '    Write-Host "ğŸ“ $($part.Name)"' >> reassemble.ps1
            echo '    $bytes = [System.IO.File]::ReadAllBytes($part.FullName)' >> reassemble.ps1
            echo '    $outStream.Write($bytes, 0, $bytes.Length)' >> reassemble.ps1
            echo '}' >> reassemble.ps1
            echo '$outStream.Close()' >> reassemble.ps1
            echo 'Write-Host "âœ… ××•×—×“ ×œ-$output"' >> reassemble.ps1
            echo 'Remove-Item part_* -Force' >> reassemble.ps1
            
            echo "âœ… × ×•×¦×¨×• ×¡×§×¨×™×¤×˜×™ reassemble"
          else
            echo "âš¡ ×§×•×‘×¥ ×§×˜×Ÿ ×-${CHUNK_SIZE}MB - ×œ× ×¦×¨×™×š ×¤×™×¦×•×œ"
          fi
          
      - name: Commit and push
        run: |
          MODEL_NAME="${{ github.event.inputs.model_name }}"
          MODEL_FOLDER="${MODEL_NAME//:/_}"
          
          git config user.name "GitHub Actions"
          git config user.email "actions@github.com"
          
          git add "$MODEL_FOLDER"
          git commit -m "ğŸ¤– Add Ollama model: $MODEL_NAME"
          git push
          
          echo "âœ… ×”××•×“×œ ×”×•×¢×œ×” ×œ-GitHub!"
          
      - name: Summary
        run: |
          MODEL_NAME="${{ github.event.inputs.model_name }}"
          MODEL_FOLDER="${MODEL_NAME//:/_}"
          
          cd "$MODEL_FOLDER"
          
          if ls part_* 1> /dev/null 2>&1; then
            echo "ğŸ“¦ ×”××•×“×œ ×¤×•×¦×œ ×œ××§×˜×¢×™×:"
            ls -lh part_*
            echo ""
            echo "×œ×”×•×¨×“×” ×•×”×ª×§× ×”:"
            echo "1. git clone --depth 1 --filter=blob:none --sparse ${{ github.repository }}"
            echo "2. cd $(basename ${{ github.repository }})"
            echo "3. git sparse-checkout set $MODEL_FOLDER"
            echo "4. cd $MODEL_FOLDER && ./reassemble.sh"
            echo "5. tar -xzf models.tar.gz -C ~/.ollama/"
          else
            SIZE=$(du -h models.tar.gz | cut -f1)
            echo "ğŸ“¦ ×”××•×“×œ ××•×›×Ÿ ×œ×”×•×¨×“×”: models.tar.gz ($SIZE)"
            echo ""
            echo "×œ×”×•×¨×“×” ×•×”×ª×§× ×”:"
            echo "1. git clone --depth 1 --filter=blob:none --sparse ${{ github.repository }}"
            echo "2. cd $(basename ${{ github.repository }})"
            echo "3. git sparse-checkout set $MODEL_FOLDER"
            echo "4. cd $MODEL_FOLDER"
            echo "5. tar -xzf models.tar.gz -C ~/.ollama/"
          fi
          
          echo ""
          echo "×œ×©×™××•×©:"
          echo "ollama run $MODEL_NAME"
