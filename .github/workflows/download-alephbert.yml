name: Download AlephBERT Models

on:
  workflow_dispatch:
    inputs:
      model_size:
        description: 'Model size to download'
        required: true
        default: 'base'
        type: choice
        options:
          - small
          - base
          - large

jobs:
  download-alephbert:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        pip install transformers torch huggingface_hub
    
    - name: Download AlephBERT model
      run: |
        python - << 'EOF'
        from transformers import BertTokenizer, BertForMaskedLM
        from huggingface_hub import snapshot_download
        import os
        
        model_size = "${{ github.event.inputs.model_size }}"
        model_name = f"onlplab/alephbert-{model_size}"
        
        print(f"ðŸ“¥ ×ž×•×¨×™×“ ××ª {model_name}...")
        
        # ×”×•×¨×“×” ×ž-HuggingFace
        try:
            # ×”×•×¨×“×ª ×”×ž×•×“×œ ×”×ž×œ×
            model_path = snapshot_download(
                repo_id=model_name,
                cache_dir="./alephbert_cache",
                local_dir=f"./alephbert_models/alephbert-{model_size}",
                local_dir_use_symlinks=False
            )
            print(f"âœ… ×”×•×¨×“×” ×”×•×©×œ×ž×”: {model_path}")
            
            # ×‘×“×™×§×” ×©×”×ž×•×“×œ ×¢×•×‘×“
            print(f"ðŸ§ª ×‘×•×“×§ ××ª ×”×ž×•×“×œ...")
            tokenizer = BertTokenizer.from_pretrained(f"./alephbert_models/alephbert-{model_size}")
            model = BertForMaskedLM.from_pretrained(f"./alephbert_models/alephbert-{model_size}")
            
            # ×‘×“×™×§×” ×‘×¡×™×¡×™×ª
            test_text = "×©×œ×•× [MASK] ×˜×•×‘"
            inputs = tokenizer(test_text, return_tensors='pt')
            outputs = model(**inputs)
            
            print(f"âœ… ×”×ž×•×“×œ ×¢×•×‘×“!")
            print(f"ðŸ“Š ×’×•×“×œ ×ž×™×œ×•×Ÿ: {tokenizer.vocab_size}")
            
        except Exception as e:
            print(f"âŒ ×©×’×™××”: {e}")
            exit(1)
        EOF
    
    - name: Create test script
      run: |
        cat > test_alephbert_downloaded.py << 'EOF'
        """
        ×‘×“×™×§×ª AlephBERT ×©×”×•×¨×“ ×ž-HuggingFace
        """
        
        from transformers import BertTokenizer, BertForMaskedLM
        import torch
        import os
        
        model_size = "${{ github.event.inputs.model_size }}"
        model_path = f"./alephbert_models/alephbert-{model_size}"
        
        print(f"ðŸ” ×˜×•×¢×Ÿ ××ª AlephBERT-{model_size}...")
        
        if not os.path.exists(model_path):
            print(f"âŒ ×”×ž×•×“×œ ×œ× × ×ž×¦× ×‘-{model_path}")
            exit(1)
        
        tokenizer = BertTokenizer.from_pretrained(model_path)
        model = BertForMaskedLM.from_pretrained(model_path)
        model.eval()
        
        print(f"âœ… ×”×ž×•×“×œ × ×˜×¢×Ÿ ×‘×”×¦×œ×—×”!")
        print(f"ðŸ“Š ×’×•×“×œ ×ž×™×œ×•×Ÿ: {tokenizer.vocab_size}")
        
        def predict_masked_word(sentence, top_k=10):
            """
            ×ž× ×‘× ×ž×™×œ×™× ×—×¡×¨×•×ª
            """
            print(f"\nðŸ“ ×ž×©×¤×˜: {sentence}")
            
            inputs = tokenizer(sentence, return_tensors='pt')
            outputs = model(**inputs)
            
            # ×ž×¦× ××ª ×ž×™×§×•× ×”-MASK
            mask_token_index = torch.where(inputs['input_ids'] == tokenizer.mask_token_id)[1]
            
            if len(mask_token_index) == 0:
                print("âŒ ×œ× × ×ž×¦× [MASK] ×‘×ž×©×¤×˜")
                return
            
            mask_token_logits = outputs.logits[0, mask_token_index, :]
            top_tokens = torch.topk(mask_token_logits, top_k, dim=1).indices[0].tolist()
            
            print(f"ðŸ† Top {top_k} ×ª×—×–×™×•×ª:")
            for i, token_id in enumerate(top_tokens, 1):
                word = tokenizer.decode([token_id])
                print(f"   {i}. {word}")
        
        # ×‘×“×™×§×•×ª
        print("\n" + "="*70)
        print("×‘×“×™×§×” 1: ×‘\"×” - ×‘×¨×•×š ×”' ××• ×‘×¨×•×š ×”×•×?")
        print("="*70)
        predict_masked_word("×œ×¤×™ ×©×”×™×” ××‘×¨×”× ×‘×˜×œ ×‘×™×—×•×“ ××•×¨ ××™×Ÿ ×¡×•×£ [MASK]")
        
        print("\n" + "="*70)
        print("×‘×“×™×§×” 2: ×ž×¦×•×ª ×‘×”×§×©×¨ ×©×œ ××•×›×œ")
        print("="*70)
        predict_masked_word("×‘×—×’ ×”×¤×¡×— ××•×›×œ×™× [MASK] ×‘×ž×§×•× ×œ×—×")
        
        print("\n" + "="*70)
        print("×‘×“×™×§×” 3: ×ž×¦×•×ª ×‘×”×§×©×¨ ×©×œ ×ª×•×¨×”")
        print("="*70)
        predict_masked_word("×¢×œ ×™×“×™ ×¢×¡×§ ×”×ª×•×¨×” ×•×”×ž×¦×•×ª × ×›×œ×œ [MASK] ×”×§×•×“×©")
        
        print("\n" + "="*70)
        print("×‘×“×™×§×” 4: ×ž×™×œ×” ×—×¡×¨×” ×¤×©×•×˜×”")
        print("="*70)
        predict_masked_word("×“× ×™ ×”×œ×š ×œ×‘×™×ª [MASK] ×”×™×•×")
        
        print("\nâœ… ×›×œ ×”×‘×“×™×§×•×ª ×”×•×©×œ×ž×•!")
        EOF
    
    - name: Test AlephBERT
      run: |
        python test_alephbert_downloaded.py
    
    - name: Package model as artifact
      run: |
        cd alephbert_models
        tar -czf alephbert-${{ github.event.inputs.model_size }}.tar.gz alephbert-${{ github.event.inputs.model_size }}
        ls -lh *.tar.gz
    
    - name: Upload model artifact
      uses: actions/upload-artifact@v4
      with:
        name: alephbert-${{ github.event.inputs.model_size }}-model
        path: alephbert_models/alephbert-${{ github.event.inputs.model_size }}.tar.gz
        retention-days: 7
    
    - name: Create summary
      run: |
        echo "## ðŸ“¦ AlephBERT Model Downloaded" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "âœ… **Model:** alephbert-${{ github.event.inputs.model_size }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“¥ ×œ×”×•×¨×“×”:" >> $GITHUB_STEP_SUMMARY
        echo "1. ×œ×š ×œ-Actions" >> $GITHUB_STEP_SUMMARY
        echo "2. ×‘×—×¨ ××ª ×”-workflow ×”×–×”" >> $GITHUB_STEP_SUMMARY
        echo "3. ×”×•×¨×“ ××ª ×”-artifact: \`alephbert-${{ github.event.inputs.model_size }}-model\`" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ”§ ×œ×”×ª×§× ×” ×ž×§×•×ž×™×ª:" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`bash" >> $GITHUB_STEP_SUMMARY
        echo "tar -xzf alephbert-${{ github.event.inputs.model_size }}.tar.gz" >> $GITHUB_STEP_SUMMARY
        echo "\`\`\`" >> $GITHUB_STEP_SUMMARY
