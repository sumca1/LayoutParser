name: Test All Ollama Models for Hebrew

on:
  workflow_dispatch:
    inputs:
      models_to_test:
        description: '×¨×©×™××ª ××•×“×œ×™× ×œ×‘×“×™×§×” (××• "all" ×œ×›×•×œ×)'
        required: false
        default: 'aya-expanse:8b,llama3.2:3b,gemma2:9b,qwen2.5:7b,mistral:7b'

jobs:
  test-ollama-models:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install requests
      
      - name: Install Ollama
        run: |
          echo "××ª×§×™×Ÿ Ollama..."
          curl -fsSL https://ollama.com/install.sh | sh
          
          # ×”×¤×¢×œ Ollama ×‘×¨×§×¢
          ollama serve > /dev/null 2>&1 &
          
          # ×”××ª×Ÿ ×©×”×©×™×¨×•×ª ×™×¢×œ×”
          sleep 5
          
          echo "âœ“ Ollama ××•×ª×§×Ÿ ×•×¨×¥"
      
      - name: Create test script
        run: |
          cat > test_all_models.py << 'EOF'
          """
          ×‘×“×™×§×” ××•×˜×•××˜×™×ª ×©×œ ×›×œ ××•×“×œ×™ Ollama ×œ×¢×‘×¨×™×ª
          """
          import requests
          import time
          import json
          import sys
          
          
          def ask_model(model_name, prompt, timeout=120):
              """×©××œ ××•×“×œ"""
              try:
                  start = time.time()
                  response = requests.post(
                      "http://localhost:11434/api/generate",
                      json={
                          "model": model_name,
                          "prompt": prompt,
                          "stream": False,
                          "options": {
                              "temperature": 0.1,
                              "num_predict": 50
                          }
                      },
                      timeout=timeout
                  )
                  elapsed = time.time() - start
                  
                  if response.status_code == 200:
                      return {
                          'success': True,
                          'response': response.json().get('response', '').strip(),
                          'time': elapsed
                      }
              except Exception as e:
                  return {'success': False, 'error': str(e)}
              
              return {'success': False, 'error': 'Unknown'}
          
          
          def test_model(model_name):
              """×‘×“×•×§ ××•×“×œ ×‘×•×“×“"""
              print(f"\n{'='*80}")
              print(f"×‘×“×™×§×ª {model_name}")
              print(f"{'='*80}\n")
              
              # ×‘×“×™×§×•×ª
              tests = [
                  {
                      'name': '××¦×•×ª - ××•×›×œ×™×',
                      'prompt': 'In Hebrew sentence "××•×›×œ×™× ××ª ×”××¦×•×ª", the word ××¦×•×ª means:\n1) ×Ö·×¦Ö¼×•Ö¹×ª (matzot-bread)\n2) ×Ö´×¦Ö°×•×•Ö¹×ª (mitzvot-commandments)\nAnswer with only number:',
                      'correct': '1'
                  },
                  {
                      'name': '××¦×•×ª - ×ª×•×¨×”',
                      'prompt': 'In Hebrew sentence "×©×•××¨×™× ××ª ×”×ª×•×¨×” ×•×”××¦×•×ª", the word ××¦×•×ª means:\n1) ×Ö·×¦Ö¼×•Ö¹×ª (matzot-bread)\n2) ×Ö´×¦Ö°×•×•Ö¹×ª (mitzvot-commandments)\nAnswer with only number:',
                      'correct': '2'
                  },
                  {
                      'name': '×‘"×” - ×‘×ª×—×™×œ×”',
                      'prompt': 'In Hebrew sentence "×‘\\"×” × ×ª×—×™×œ ××ª ×”×¢×‘×•×“×”", ×‘\\"×” means:\n1) ×‘×¨×•×š ×”\' (Baruch Hashem)\n2) ×‘×¨×•×š ×”×•× (Baruch Hu)\nAnswer with only number:',
                      'correct': '1'
                  },
                  {
                      'name': '×‘"×” - ×‘×¡×•×£',
                      'prompt': 'In Hebrew sentence "××•×¨ ××™×Ÿ ×¡×•×£ ×‘\\"×”", ×‘\\"×” means:\n1) ×‘×¨×•×š ×”\' (Baruch Hashem)\n2) ×‘×¨×•×š ×”×•× (Baruch Hu)\nAnswer with only number:',
                      'correct': '2'
                  }
              ]
              
              correct = 0
              total = len(tests)
              total_time = 0
              
              for test in tests:
                  print(f"  {test['name']}...", end=' ')
                  result = ask_model(model_name, test['prompt'])
                  
                  if result['success']:
                      answer = result['response']
                      parsed = '1' if answer.startswith('1') else '2' if answer.startswith('2') else '?'
                      is_correct = parsed == test['correct']
                      
                      if is_correct:
                          correct += 1
                          print(f"âœ“ ({result['time']:.1f}s)")
                      else:
                          print(f"âœ— ×¢× ×” {parsed}, ×¦×¨×™×š {test['correct']} ({result['time']:.1f}s)")
                      
                      total_time += result['time']
                  else:
                      print(f"âœ— ×©×’×™××”: {result.get('error', 'Unknown')}")
              
              accuracy = 100 * correct / total
              avg_time = total_time / total if total > 0 else 0
              
              print(f"\n  ×ª×•×¦××”: {correct}/{total} ({accuracy:.0f}%)")
              print(f"  ×–××Ÿ ×××•×¦×¢: {avg_time:.1f}s")
              
              return {
                  'model': model_name,
                  'correct': correct,
                  'total': total,
                  'accuracy': accuracy,
                  'avg_time': avg_time
              }
          
          
          def main():
              # ×§×‘×œ ×¨×©×™××ª ××•×“×œ×™×
              models_str = sys.argv[1] if len(sys.argv) > 1 else 'aya-expanse:8b,llama3.2:3b'
              models = [m.strip() for m in models_str.split(',')]
              
              print("=" * 80)
              print("×‘×“×™×§×” ××•×˜×•××˜×™×ª ×©×œ ××•×“×œ×™ Ollama ×œ×¢×‘×¨×™×ª")
              print("=" * 80)
              print(f"\n××•×“×œ×™× ×œ×‘×“×™×§×”: {len(models)}")
              for model in models:
                  print(f"  â€¢ {model}")
              
              results = []
              
              # ×”×•×¨×“ ×•×‘×“×•×§ ×›×œ ××•×“×œ
              for model in models:
                  print(f"\n{'='*80}")
                  print(f"××•×¨×™×“ {model}...")
                  print(f"{'='*80}")
                  
                  import subprocess
                  try:
                      subprocess.run(['ollama', 'pull', model], check=True, timeout=600)
                      print(f"âœ“ {model} ×”×•×¨×“ ×‘×”×¦×œ×—×”")
                  except Exception as e:
                      print(f"âœ— ×©×’×™××” ×‘×”×•×¨×“×ª {model}: {e}")
                      continue
                  
                  # ×‘×“×•×§
                  result = test_model(model)
                  results.append(result)
                  
                  # ××—×§ ××•×“×œ ×œ×¤× ×™ ×”×‘× (×œ×—×¡×•×š ××§×•×)
                  try:
                      subprocess.run(['ollama', 'rm', model], check=False)
                  except:
                      pass
              
              # ×¡×™×›×•×
              print("\n" + "=" * 80)
              print("×¡×™×›×•× ×›×œ ×”××•×“×œ×™×")
              print("=" * 80)
              print()
              
              # ××™×™×Ÿ ×œ×¤×™ ×“×™×•×§
              results.sort(key=lambda x: (-x['accuracy'], x['avg_time']))
              
              print(f"{'×“×™×¨×•×’':<5} {'××•×“×œ':<25} {'×“×™×•×§':<10} {'×–××Ÿ ×××•×¦×¢':<12} {'×¦×™×•×Ÿ'}")
              print("-" * 80)
              
              for i, r in enumerate(results, 1):
                  medal = 'ğŸ¥‡' if i == 1 else 'ğŸ¥ˆ' if i == 2 else 'ğŸ¥‰' if i == 3 else '  '
                  grade = 'âœ“âœ“âœ“' if r['accuracy'] >= 75 else 'âœ“âœ“' if r['accuracy'] >= 50 else 'âœ“' if r['accuracy'] >= 25 else 'âœ—'
                  
                  print(f"{medal} {i:<3} {r['model']:<25} {r['accuracy']:.0f}%{'':<6} {r['avg_time']:.1f}s{'':<8} {grade}")
              
              # ×”××œ×¦×”
              if results:
                  best = results[0]
                  print()
                  print(f"ğŸ† ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨: {best['model']}")
                  print(f"   ×“×™×•×§: {best['accuracy']:.0f}%")
                  print(f"   ×–××Ÿ: {best['avg_time']:.1f}s")
                  
                  if best['accuracy'] >= 75:
                      print(f"   ×”××œ×¦×”: âœ“âœ“âœ“ ××¦×•×™×Ÿ ×œ×¢×‘×¨×™×ª!")
                  elif best['accuracy'] >= 50:
                      print(f"   ×”××œ×¦×”: âœ“âœ“ ×¡×‘×™×¨, ××‘×œ DictaBERT ×¢×“×™×£")
                  else:
                      print(f"   ×”××œ×¦×”: âœ— ×œ× ××¡×¤×™×§ ×˜×•×‘")
              
              # ×©××•×¨ ×ª×•×¦××•×ª
              with open('ollama_test_results.json', 'w', encoding='utf-8') as f:
                  json.dump(results, f, ensure_ascii=False, indent=2)
              
              print("\nâœ“ ×ª×•×¦××•×ª × ×©××¨×• ×‘-ollama_test_results.json")
          
          
          if __name__ == '__main__':
              main()
          EOF
      
      - name: Run tests
        run: |
          python test_all_models.py "${{ github.event.inputs.models_to_test }}"
      
      - name: Upload results
        uses: actions/upload-artifact@v3
        with:
          name: ollama-test-results
          path: ollama_test_results.json
      
      - name: Comment results on PR (if PR exists)
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('ollama_test_results.json', 'utf8'));
            
            let comment = '## ğŸ¤– ×ª×•×¦××•×ª ×‘×“×™×§×ª ××•×“×œ×™ Ollama ×œ×¢×‘×¨×™×ª\n\n';
            comment += '| ×“×™×¨×•×’ | ××•×“×œ | ×“×™×•×§ | ×–××Ÿ ×××•×¦×¢ | ×¦×™×•×Ÿ |\n';
            comment += '|-------|------|------|------------|------|\n';
            
            results.sort((a, b) => b.accuracy - a.accuracy);
            
            results.forEach((r, i) => {
              const medal = i === 0 ? 'ğŸ¥‡' : i === 1 ? 'ğŸ¥ˆ' : i === 2 ? 'ğŸ¥‰' : '';
              const grade = r.accuracy >= 75 ? 'âœ“âœ“âœ“' : r.accuracy >= 50 ? 'âœ“âœ“' : 'âœ“';
              comment += `| ${medal} ${i+1} | ${r.model} | ${r.accuracy.toFixed(0)}% | ${r.avg_time.toFixed(1)}s | ${grade} |\n`;
            });
            
            if (results.length > 0) {
              const best = results[0];
              comment += `\n### ğŸ† ×”××•×“×œ ×”×˜×•×‘ ×‘×™×•×ª×¨: ${best.model}\n`;
              comment += `- **×“×™×•×§**: ${best.accuracy.toFixed(0)}%\n`;
              comment += `- **×–××Ÿ ×××•×¦×¢**: ${best.avg_time.toFixed(1)}s\n`;
            }
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
